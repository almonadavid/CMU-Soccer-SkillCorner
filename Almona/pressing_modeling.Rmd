---
title: "Forced Turnover Prediction: Evaluating Pressing Effectiveness in Soccer"
author: "[enter name]"
date: "2025-07-16"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Here, we are analyzing pressing sequences in soccer to predict whether they will result in a forced turnover within 5 seconds after the pressing sequence starts. Let's start by loading our data and doing some basic preprocessing.


## Libraries
```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(tidymodels)
library(stacks)
library(xgboost)
library(caret)
library(glmnet)
library(tictoc)
theme_set(theme_light())
```


## Data Loading and Preparation
Our dataset contains pressing sequences from 10 MLS matches in the 2023 season. Each row represents a pressing sequence with various spatio-temporal variables.

```{r load-data, message=FALSE}
pressing_data <- read_csv("results/all_games_pressing_sequences.csv") |>
  filter( # remove coordinates outside field boundaries
    between(ball_carrier_x, -52.5, 52.5),
    between(ball_carrier_y, -34, 34)
  ) |> 
  select(-match_id:-player_in_possession_name, -game_id) |>
  mutate_if(is.character, as.factor) |> 
  mutate_if(is.logical, as.factor) |> 
  as.data.table()

# exclude pressing sequence that last just 1 frame (0.1s), could be noise
# pressing_data <- pressing_data[sequence_duration_frames > 1]
```
View data structure
```{r}
# head(pressing_data)
glimpse(pressing_data)
```


## Exploratory Data Analysis
```{r velocity-distance-plot}
ggplot(pressing_data, aes(x = avg_approach_velocity, y = dist_to_attacking_goal, 
                         color = forced_turnover_within_5s)) +
  geom_point(alpha = 0.6) +
  labs(
    x = "Average Approach Velocity",
    y = "Distance to Attacking Goal (m)",
    color = "Forced Turnover",
  )
```

```{r start-type-plot}
ggplot(pressing_data, aes(start_type, fill = forced_turnover_within_5s)) +
  geom_bar(position = "fill") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r start-type-plot-2}
ggplot(pressing_data, aes(start_type, fill = forced_turnover_within_5s)) +
  geom_bar(position = "dodge") +
  labs(y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r field-third-plot}
ggplot(pressing_data, aes(poss_third_start, fill = forced_turnover_within_5s)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(
    y = "Proportion", 
    x = "Possession Third", 
    fill = "Turnover"
  )
```


## Build a model with `tidymodels`
Create training and testing sets

```{r}
set.seed(123)
pressing_split <- initial_split(pressing_data, strata = forced_turnover_within_5s)
pressing_train <- training(pressing_split)
pressing_test <- testing(pressing_split)
```

### XGBoost Model
```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), 
  min_n = tune(), 
  loss_reduction = tune(),
  sample_size = tune(), 
  mtry = tune(), 
  learn_rate = tune()
) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

xgb_spec
```
We're tuning 6 hyperparameters to find the optimal model configuration. This will help prevent overfitting while maximizing predictive performance.

### Hyperparameter grid
```{r}
xgb_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), pressing_train),
  learn_rate(),
  size = 20 # you can change this number
  
)

xgb_grid
```

### Workflow Setup
```{r}
xgb_wf <- workflow() |> 
  add_formula(forced_turnover_within_5s ~ .) |> 
  add_model(xgb_spec)

xgb_wf
```

### Cross-Validation Folds
10-fold cross-validation
```{r}
pressing_folds <- vfold_cv(pressing_train, strata = forced_turnover_within_5s)

pressing_folds
```


###  Hyperparamter tuning
Now we'll train our XGBoost model with all 20 hyperparameter combinations across our 10 cross-validation folds. This will take a few minutes to run.

```{r xgb-tuning, cache=TRUE, warning=FALSE}
doParallel::registerDoParallel() # to make this process faster

set.seed(456)
tic()
xgb_res <- tune_grid(
  xgb_wf,
  resamples = pressing_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE) # save for roc curve
)
toc()

xgb_res

```

**Explain:** We trained 200 models (20 hyperparameter combinations Ã— 10 folds) and evaluated each one. The save_pred = TRUE option stores all the predictions so we can create detailed performance plots later.

## Explore Results
```{r}
xgb_res |> 
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  select(mean, mtry:sample_size) |> 
  pivot_longer(mtry:sample_size, 
               names_to = "parameter",
               values_to = "value") |> 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x")
```


## Best Performing Model
```{r show-best}
show_best(xgb_res, metric = "roc_auc")
```
```{r}
best_auc <- select_best(xgb_res, metric = "roc_auc")

best_auc
```

### Finalize workflow
```{r}
final_xgb <- finalize_workflow(xgb_wf, best_auc )

final_xgb
```

## Variable Importance
Which features are most important for predicting pressing success?
```{r}
library(vip)

final_xgb |> 
  fit(data = pressing_train) |> 
  extract_fit_parsnip() |> 
  vip()
```

## Final Evaluation
Now let's see how our model performs on unseen data:
```{r}
final_res <- last_fit(final_xgb, pressing_split)

final_res |> 
  collect_metrics() 
```

### Confusion matrix
```{r}
final_res |> 
  collect_predictions() |> 
  conf_mat(forced_turnover_within_5s, .pred_class)
```

### ROC cuve
```{r}
final_res |> 
  collect_predictions() |> 
  roc_curve(forced_turnover_within_5s, .pred_FALSE) |> 
  autoplot()
```
## Save Results
```{r}
saveRDS(final_res, "final_results.rds")
saveRDS(final_xgb, "final_model.rds")
saveRDS(pressing_train, "pressing_train.rds")
```


## Next steps
Now that we have an XGBoost model, we plan to compare it with other models.
e.g logistic, random forest, lightGBM


